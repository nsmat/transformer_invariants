{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch_geometric as tg\n",
    "\n",
    "from models.se3_transformer import Se3EquivariantTransformer\n",
    "from utils.load_md17 import load_md17\n",
    "import e3nn\n",
    "\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:14.012194300Z",
     "start_time": "2023-08-14T19:42:08.273505600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MD17TransformerTask(pl.LightningModule):\n",
    "    def __init__(self, energy_model: torch.nn.Module, lr=1e-3, force_loss_weight=500):\n",
    "        super().__init__()\n",
    "        self.energy_model = energy_model\n",
    "        self.lr = lr\n",
    "        self.force_loss_weight = force_loss_weight\n",
    "\n",
    "        self.energy_train_metric = torchmetrics.MeanAbsoluteError()\n",
    "        self.energy_valid_metric = torchmetrics.MeanAbsoluteError()\n",
    "        self.energy_test_metric = torchmetrics.MeanAbsoluteError()\n",
    "        self.force_train_metric = torchmetrics.MeanAbsoluteError()\n",
    "        self.force_valid_metric = torchmetrics.MeanAbsoluteError()\n",
    "        self.force_test_metric = torchmetrics.MeanAbsoluteError()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_energy_normalisers(dataset):\n",
    "        sum_energies = 0\n",
    "        total_nodes = 0\n",
    "        force_scales = 0\n",
    "\n",
    "        for graph in dataset:\n",
    "            total_nodes += graph.num_nodes\n",
    "            sum_energies += graph.energy\n",
    "            force_scales += torch.linalg.vector_norm(graph.force, dim=1).sum()\n",
    "\n",
    "        mean = sum_energies / total_nodes\n",
    "        std = force_scales / total_nodes\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.pos = torch.autograd.Variable(graph.pos, requires_grad=True)\n",
    "        predicted_energy = self.energy_model(graph).sum(1)\n",
    "        expected_outputs = torch.ones_like(predicted_energy, requires_grad=True)\n",
    "\n",
    "        predicted_force = -1 * torch.autograd.grad(predicted_energy,\n",
    "                                                   graph.pos,\n",
    "                                                   grad_outputs=expected_outputs,\n",
    "                                                   create_graph=True,\n",
    "                                                   retain_graph=True,\n",
    "                                                   # set_detect_anomaly=True\n",
    "                                                   )[0]\n",
    "\n",
    "        # predicted_energy = predicted_energy.squeeze(-1)\n",
    "\n",
    "        return predicted_energy, predicted_force\n",
    "\n",
    "    def energy_and_force_loss(self, graph, energy, force):\n",
    "        energy_loss = torch.nn.functional.mse_loss(energy, graph.energy)\n",
    "        force_loss = torch.nn.functional.mse_loss(force, graph.force)\n",
    "        loss = energy_loss + self.force_loss_weight * force_loss\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, graph):\n",
    "        energy, force = self(graph)\n",
    "        loss = self.energy_and_force_loss(graph, energy, force)\n",
    "        self.energy_train_metric(energy, graph.energy)\n",
    "        self.force_train_metric(force, graph.force)\n",
    "\n",
    "        cur_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n",
    "        self.log(\"lr\", cur_lr, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"Energy train MAE\", self.energy_train_metric, prog_bar=True)\n",
    "        self.log(\"Force train MAE\", self.force_train_metric, prog_bar=True)\n",
    "\n",
    "    @torch.inference_mode(False)\n",
    "    def validation_step(self, graph, batch_idx):\n",
    "        energy, force = self.forward(graph)\n",
    "        self.energy_valid_metric(energy, graph.energy)\n",
    "        self.force_valid_metric(force, graph.force)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"Energy valid MAE\", self.energy_valid_metric, prog_bar=True)\n",
    "        self.log(\"Force valid MAE\", self.force_valid_metric, prog_bar=True)\n",
    "\n",
    "    def test_step(self, graph, batch_idx):\n",
    "        energy, force = self.forward(graph)\n",
    "        self.energy_test_metric(energy, graph.energy)\n",
    "        self.force_test_metric(force, graph.force)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"Energy test MAE\", self.energy_test_metric, prog_bar=True)\n",
    "        self.log(\"Force test MAE\", self.force_test_metric, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        num_steps = self.trainer.estimated_stepping_batches\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps)\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler_config]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:14.066555700Z",
     "start_time": "2023-08-14T19:42:14.016192400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### Prototype workflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:14.073568300Z",
     "start_time": "2023-08-14T19:42:14.059559500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\torch\\jit\\_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "energy_model = Se3EquivariantTransformer.construct_from_number_of_channels_and_lmax(num_channels=8,\n",
    "                                                                             l_max=2,\n",
    "                                                                             num_features=9,\n",
    "                                                                             num_attention_layers=4,\n",
    "                                                                             num_attention_heads=4,\n",
    "                                                                             radial_network_hidden_units=32\n",
    "                                                                             )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:19.639430300Z",
     "start_time": "2023-08-14T19:42:14.079571200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "radius=2\n",
    "\n",
    "data = load_md17(dataset_name='aspirin CCSD', dataset_dir='../real_datasets', radius=radius)\n",
    "\n",
    "train_dataloader = tg.loader.DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = tg.loader.DataLoader(data['validation'], batch_size=batch_size)\n",
    "test_dataloader = tg.loader.DataLoader(data['test'], batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:19.690185200Z",
     "start_time": "2023-08-14T19:42:19.641428400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-837711.3125]), tensor(2015.1858))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD17TransformerTask.compute_energy_normalisers(dataset=data['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:20.128909900Z",
     "start_time": "2023-08-14T19:42:19.674177300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)\n",
    "task = MD17TransformerTask(energy_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:20.288736100Z",
     "start_time": "2023-08-14T19:42:20.130874500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1a7ff016100>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:42:20.300259400Z",
     "start_time": "2023-08-14T19:42:20.286735500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                | Type                      | Params\n",
      "------------------------------------------------------------------\n",
      "0 | energy_model        | Se3EquivariantTransformer | 208   \n",
      "1 | energy_train_metric | MeanAbsoluteError         | 0     \n",
      "2 | energy_valid_metric | MeanAbsoluteError         | 0     \n",
      "3 | energy_test_metric  | MeanAbsoluteError         | 0     \n",
      "4 | force_train_metric  | MeanAbsoluteError         | 0     \n",
      "5 | force_valid_metric  | MeanAbsoluteError         | 0     \n",
      "6 | force_test_metric   | MeanAbsoluteError         | 0     \n",
      "------------------------------------------------------------------\n",
      "208       Trainable params\n",
      "0         Non-trainable params\n",
      "208       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89c2c4cbcc974b6fa3b076f647850dbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38d9ab043bd64a1d9e1dabaf8ab4643e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([96])\n",
      "torch.Size([96, 1])\n",
      "torch.Size([96])\n",
      "torch.Size([96, 1])\n",
      "torch.Size([106])\n",
      "torch.Size([106, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([104])\n",
      "torch.Size([104, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([98])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(task, train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:45:27.989712Z",
     "start_time": "2023-08-14T19:42:20.302261100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for b in train_dataloader:\n",
    "    b = b.clone()\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:45:28.045233800Z",
     "start_time": "2023-08-14T19:45:28.006719900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "out = energy_model(b)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-14T19:45:28.017231700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = out.sum().backward()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor([[1, 2], [3, 4]]).view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
