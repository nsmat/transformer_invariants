{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T07:42:03.547885200Z",
     "start_time": "2023-08-05T07:42:03.530515200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric as tg\n",
    "\n",
    "import torch\n",
    "import e3nn\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from models.attention_mechanisms import Se3AttentionHead\n",
    "from utils.load_md17 import load_md17\n",
    "from utils.transforms import EuclideanInformationTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dd = load_md17('aspirin CCSD', './../real_datasets/MD17', 2)\n",
    "data = dd['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:28:56.124820300Z",
     "start_time": "2023-08-05T07:28:56.092557100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:28:56.352196700Z",
     "start_time": "2023-08-05T07:28:56.245134200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def make_3d_rotation_matrix(alpha, beta, gamma):\n",
    "    rot_z = torch.tensor([[torch.cos(alpha), -torch.sin(alpha), 0],\n",
    "                          [torch.sin(alpha),  torch.cos(alpha), 0],\n",
    "                          [0,                0,                1]])\n",
    "\n",
    "    rot_y = torch.tensor([[torch.cos(beta), 0, torch.sin(beta)],\n",
    "                          [0,               1,               0],\n",
    "                          [-torch.sin(beta),0, torch.cos(beta)]])\n",
    "    rot_x = torch.tensor([[1, 0,                0               ],\n",
    "                          [0, torch.cos(gamma), -torch.sin(beta)],\n",
    "                          [0 ,torch.sin(gamma), torch.cos(gamma)]]\n",
    "                         )\n",
    "\n",
    "    full_rotation_matrix = rot_z @ rot_y @ rot_x\n",
    "\n",
    "    return full_rotation_matrix\n",
    "\n",
    "\n",
    "def rotate_graph(graph: tg.data.Data, alpha, beta, gamma):\n",
    "    \"\"\"Return a copy of the graph with all geometric quantities rotated\n",
    "    according to the spherical angles alpha, beta, gamma\"\"\"\n",
    "\n",
    "    out_graph = graph.clone()\n",
    "    rotation_matrix = make_3d_rotation_matrix(alpha, beta, gamma)\n",
    "\n",
    "    out_graph.pos =(rotation_matrix @ out_graph.pos.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Rederive the relative positions etc\n",
    "    transform = EuclideanInformationTransform\n",
    "    out_graph = transform(out_graph)\n",
    "\n",
    "    return out_graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:36:38.922777Z",
     "start_time": "2023-08-05T07:36:38.915763200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "graph = data[0]\n",
    "alpha, beta, gamma = map(torch.tensor, (np.pi/2., 0, 0))\n",
    "rot = make_3d_rotation_matrix(alpha, beta, gamma)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:29:53.258193300Z",
     "start_time": "2023-08-05T07:29:53.244401700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niksm\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\torch\\jit\\_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "feature_irreps = e3nn.o3.Irreps(\"10x0e + 10x1e + 10x2e\")\n",
    "geometric_irreps = e3nn.o3.Irreps(\"3x0e+3x1e+3x2e\")\n",
    "output_irreps = e3nn.o3.Irreps(\"10x0e+10x1e+10x2e\")\n",
    "\n",
    "att = Se3AttentionHead(num_attention_layers=3,\n",
    "                       feature_input_repr = feature_irreps,\n",
    "                       feature_output_repr=feature_irreps,\n",
    "                       geometric_repr=geometric_irreps,\n",
    "                       hidden_feature_repr=feature_irreps,\n",
    "                       key_and_query_irreps=feature_irreps\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:42:07.398193800Z",
     "start_time": "2023-08-05T07:42:06.010621800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect last dimension for x",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43matt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mz\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelative_positions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistances\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\transformer_invariants\\models\\attention_mechanisms.py:105\u001B[0m, in \u001B[0;36mSe3AttentionHead.forward\u001B[1;34m(self, edge_index, node_features, edge_features, distances)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, edge_index, node_features, edge_features, distances):\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layers:\n\u001B[1;32m--> 105\u001B[0m         node_features \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m node_features\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\transformer_invariants\\models\\attention_mechanisms.py:38\u001B[0m, in \u001B[0;36mSe3EquivariantAttentionMechanism.forward\u001B[1;34m(self, edge_index, node_features, edge_features, distances)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, edge_index, node_features, edge_features, distances):\n\u001B[1;32m---> 38\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery_network(node_features)\n\u001B[0;32m     40\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_network(node_features, edge_features, distances)\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\transformer_invariants\\models\\tensor_field_networks.py:57\u001B[0m, in \u001B[0;36mRadiallyParamaterisedTensorProduct.forward\u001B[1;34m(self, feature_spherical_harmonics, geometric_spherical_harmonics, norm)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;03mApplies a tensor product, paramaterised by the norm, between vectors of spherical\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    harmonic coefficients representing features, one the one hand, and and\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m    spherical harmonics representing geometric information on the other.\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     56\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mradial_net(norm)  \u001B[38;5;66;03m# Obtain the weights as a function of the norm\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor_product\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_spherical_harmonics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mgeometric_spherical_harmonics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Documents\\CodeForUni\\venvs\\transformer_invariants\\lib\\site-packages\\e3nn\\o3\\_tensor_product\\_tensor_product.py:537\u001B[0m, in \u001B[0;36mTensorProduct.forward\u001B[1;34m(self, x, y, weight)\u001B[0m\n\u001B[0;32m    514\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, y, weight: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    515\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Evaluate :math:`w x \\otimes y`.\u001B[39;00m\n\u001B[0;32m    516\u001B[0m \n\u001B[0;32m    517\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;124;03m        tensor of shape ``(..., irreps_out.dim)``\u001B[39;00m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 537\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in1_dim, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncorrect last dimension for x\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in2_dim, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncorrect last dimension for y\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;66;03m# - PROFILER - with torch.autograd.profiler.record_function(self._profiling_str):\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Incorrect last dimension for x"
     ]
    }
   ],
   "source": [
    "att.forward(graph.edge_index.unsqueeze(-1), graph.z.unsqueeze(1), graph.relative_positions.unsqueeze(-1), graph.distances.unsqueeze(-1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T07:58:27.007466300Z",
     "start_time": "2023-08-05T07:58:26.453238100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We need to create a test graph and associate node/edge features to it\n",
    "\n",
    "g = nx.DiGraph()\n",
    "vertices = np.arange(4)\n",
    "edges = [(0, 1),\n",
    "         (1, 2),\n",
    "         (2, 1),\n",
    "         (2, 3)]\n",
    "\n",
    "for v in vertices:\n",
    "    g.add_node(v)\n",
    "\n",
    "for e in edges:\n",
    "    g.add_edge(e)\n",
    "\n",
    "node_features = {0: {'z': 0, 'x': [0, 0]},\n",
    "                 1: {'z': 0, 'x': [0, 0]}}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'distances'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 7\u001B[0m\n\u001B[0;32m      3\u001B[0m random_geometric \u001B[38;5;241m=\u001B[39m geometric_irreps\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      5\u001B[0m distances \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m1.\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# Add a batch dimension and a node dimension\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43matt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_geometric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'distances'"
     ]
    }
   ],
   "source": [
    "# We need to add batch dimensions\n",
    "random_features = feature_irreps.randn(1, -1).unsqueeze(0)\n",
    "random_geometric = geometric_irreps.randn(1, -1).unsqueeze(0)\n",
    "\n",
    "distances = torch.tensor(1.).unsqueeze(0).unsqueeze(0)  # Add a batch dimension and a node dimension\n",
    "\n",
    "output = att.forward(random_features, random_geometric, distances)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_equivariance(rptp, 100, feature_irreps, geometric_irreps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_invariants",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
